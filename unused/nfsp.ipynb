{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nUsing TensorFlow backend.\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\n"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import rlcard\n",
    "import torch\n",
    "from rlcard.agents import NFSPAgentPytorch as NFSPAgent\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.utils import set_global_seed, tournament\n",
    "from rlcard.utils import Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make environment\n",
    "env = rlcard.make('no-limit-holdem', config={'seed': 0})\n",
    "eval_env = rlcard.make('no-limit-holdem', config={'seed': 0})\n",
    "\n",
    "env.player_num = 2\n",
    "eval_env.player_num = env.player_num\n",
    "\n",
    "# Set the iterations numbers and how frequently we evaluate the performance\n",
    "evaluate_every = 10000\n",
    "evaluate_num = 1000\n",
    "episode_num = 1000000\n",
    "\n",
    "# The intial memory size\n",
    "memory_init_size = 1000\n",
    "\n",
    "# Train the agent every X steps\n",
    "train_every = 64\n",
    "\n",
    "# The paths for saving the logs and learning curves\n",
    "log_dir = './experiments/nolimit_holdem_nfsp_result/'\n",
    "save_dir = 'models/nolimit_holdem_nfsp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Initial model not found. \n"
    }
   ],
   "source": [
    "\n",
    "# Set up the agents\n",
    "agents = []\n",
    "for i in range(env.player_num):\n",
    "    agent = NFSPAgent(scope='nfsp' + str(i),\n",
    "                      action_num=env.action_num,\n",
    "                      state_shape=env.state_shape,\n",
    "                      hidden_layers_sizes=[128,128],\n",
    "                      min_buffer_size_to_learn=memory_init_size,\n",
    "                      q_replay_memory_init_size=memory_init_size,\n",
    "                      train_every=train_every,\n",
    "                      q_train_every = train_every,\n",
    "                      q_mlp_layers=[128,128],\n",
    "                    device=torch.device('cpu'))\n",
    "    agents.append(agent)\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load('models\\\\initial_model\\\\model.pth')\n",
    "    for agent in agents:\n",
    "        agent.load(checkpoint)\n",
    "    random_agent = NFSPAgent(scope='nfsp' + str(i),\n",
    "                      action_num=env.action_num,\n",
    "                      state_shape=env.state_shape,\n",
    "                      hidden_layers_sizes=[128,128],\n",
    "                      min_buffer_size_to_learn=memory_init_size,\n",
    "                      q_replay_memory_init_size=memory_init_size,\n",
    "                      train_every=train_every,\n",
    "                      q_train_every = train_every,\n",
    "                      q_mlp_layers=[128,128],\n",
    "                    device=torch.device('cpu'))\n",
    "    random_agent.load(checkpoint)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print('Initial model not found. ')\n",
    "    random_agent = RandomAgent(action_num=eval_env.action_num)\n",
    "except RuntimeError:\n",
    "    print('Initial model not found. ')\n",
    "    random_agent = RandomAgent(action_num=eval_env.action_num)\n",
    "\n",
    "env.set_agents(agents)\n",
    "eval_env.set_agents([agents[0]]+ [random_agent for i in range(env.player_num -1)])\n",
    "logger = Logger(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7b44d049cfca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Evaluate the performance. Play with random agents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtournament\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Close files in the logger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\somat\\Documents\\GitHub\\holdem_rl\\rlcard\\utils\\logger.py\u001b[0m in \u001b[0;36mlog_performance\u001b[1;34m(self, timestep, reward)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreward\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         '''\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'timestep'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reward'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-env\\lib\\csv.py\u001b[0m in \u001b[0;36mwriterow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(episode_num):\n",
    "\n",
    "        # First sample a policy for the episode\n",
    "    for agent in agents:\n",
    "        agent.sample_episode_policy()\n",
    "\n",
    "        # Generate data from the environment\n",
    "    trajectories, _ = env.run(is_training=True)\n",
    "\n",
    "     # Feed transitions into agent memory, and train the agent\n",
    "    for i in range(env.player_num):\n",
    "        for ts in trajectories[i]:\n",
    "            agents[i].feed(ts)\n",
    "\n",
    "        # Evaluate the performance. Play with random agents.\n",
    "    if episode % evaluate_every == 0:\n",
    "        logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
    "\n",
    "    # Close files in the logger\n",
    "logger.close_files()\n",
    "\n",
    "    # Plot the learning curve\n",
    "logger.plot('NFSP')\n",
    "\n",
    "    # Save model\n",
    "save_dir = 'models/nolimit_holdem_nfsp_pytorch'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "state_dict = {}\n",
    "for agent in agents:\n",
    "    state_dict.update(agent.get_state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_dir = 'models/nolimit_holdem_nfsp_pytorch'\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "state_dict = {}\n",
    "for agent in agents:\n",
    "    state_dict.update(agent.get_state_dict())\n",
    "torch.save(state_dict, os.path.join(save_dir, 'model.pth'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bit8c6495d42d63455699a3c1e7b3c8e30a",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}