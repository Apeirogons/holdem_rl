{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nUsing TensorFlow backend.\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From C:\\Users\\somat\\Anaconda3\\envs\\tensorflow-env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\n"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import rlcard\n",
    "import torch\n",
    "from rlcard.agents import NFSPAgentPytorch as NFSPAgent\n",
    "from rlcard.agents import RandomAgent\n",
    "from rlcard.utils import set_global_seed, tournament\n",
    "from rlcard.utils import Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make environment\n",
    "env = rlcard.make('no-limit-holdem', config={'seed': 0})\n",
    "eval_env = rlcard.make('no-limit-holdem', config={'seed': 0})\n",
    "\n",
    "env.player_num = 2\n",
    "eval_env.player_num = env.player_num\n",
    "\n",
    "# Set the iterations numbers and how frequently we evaluate the performance\n",
    "evaluate_every = 10000\n",
    "evaluate_num = 1000\n",
    "episode_num = 1000000\n",
    "\n",
    "# The intial memory size\n",
    "memory_init_size = 1000\n",
    "\n",
    "# Train the agent every X steps\n",
    "train_every = 64\n",
    "\n",
    "# The paths for saving the logs and learning curves\n",
    "log_dir = './experiments/nolimit_holdem_nfsp_result/'\n",
    "save_dir = 'models/nolimit_holdem_nfsp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the agents\n",
    "agents = []\n",
    "for i in range(env.player_num):\n",
    "    agent = NFSPAgent(scope='nfsp' + str(i),\n",
    "                      action_num=env.action_num,\n",
    "                      state_shape=env.state_shape,\n",
    "                      hidden_layers_sizes=[128,128],\n",
    "                      min_buffer_size_to_learn=memory_init_size,\n",
    "                      q_replay_memory_init_size=memory_init_size,\n",
    "                      train_every=train_every,\n",
    "                      q_train_every = train_every,\n",
    "                      q_mlp_layers=[128,128],\n",
    "                    device=torch.device('cpu'))\n",
    "    agents.append(agent)\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load('models\\\\initial_model\\\\model.pth')\n",
    "    for agent in agents:\n",
    "        agent.load(checkpoint)\n",
    "    random_agent = NFSPAgent(scope='nfsp' + str(i),\n",
    "                      action_num=env.action_num,\n",
    "                      state_shape=env.state_shape,\n",
    "                      hidden_layers_sizes=[128,128],\n",
    "                      min_buffer_size_to_learn=memory_init_size,\n",
    "                      q_replay_memory_init_size=memory_init_size,\n",
    "                      train_every=train_every,\n",
    "                      q_train_every = train_every,\n",
    "                      q_mlp_layers=[128,128],\n",
    "                    device=torch.device('cpu'))\n",
    "    random_agent.load(checkpoint)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print('Initial model not found. ')\n",
    "    random_agent = RandomAgent(action_num=eval_env.action_num)\n",
    "\n",
    "\n",
    "env.set_agents(agents)\n",
    "eval_env.set_agents([agents[0]]+ [random_agent for i in range(env.player_num -1)])\n",
    "logger = Logger(log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n----------------------------------------\n  timestep     |  4\n  reward       |  2.9514\n----------------------------------------\nINFO - Agent nfsp0_dqn, step 1000, rl-loss: 4075.569580078125\nINFO - Copied model parameters to target network.\nINFO - Agent nfsp1_dqn, step 1000, rl-loss: 3952.1650390625\nINFO - Copied model parameters to target network.\nINFO - Agent nfsp0_dqn, step 15656, rl-loss: 3312.071044921875\n----------------------------------------\n  timestep     |  31502\n  reward       |  5.3046\n----------------------------------------\nINFO - Agent nfsp1_dqn, step 29928, rl-loss: 3093.855224609375\n----------------------------------------\n  timestep     |  59015\n  reward       |  3.4628\n----------------------------------------\nINFO - Agent nfsp1_dqn, step 43752, rl-loss: 2472.64501953125\n----------------------------------------\n  timestep     |  85983\n  reward       |  5.3793\n----------------------------------------\nINFO - Agent nfsp1_dqn, step 58408, rl-loss: 3250.90478515625\n----------------------------------------\n  timestep     |  114575\n  reward       |  4.4552\n----------------------------------------\nINFO - Agent nfsp1_dqn, step 65000, rl-loss: 2966.26806640625\nINFO - Copied model parameters to target network.\nINFO - Agent nfsp0_dqn, step 65000, rl-loss: 3524.22607421875\nINFO - Copied model parameters to target network.\nINFO - Agent nfsp1, step 73664, sl-loss: 1.3337393999099731\n----------------------------------------\n  timestep     |  144523\n  reward       |  2.2175\n----------------------------------------\nINFO - Agent nfsp0, step 86528, sl-loss: 1.2566959857940674\n----------------------------------------\n  timestep     |  176082\n  reward       |  1.0753\n----------------------------------------\nINFO - Agent nfsp1, step 105792, sl-loss: 1.3214492797851562\n----------------------------------------\n  timestep     |  208552\n  reward       |  3.0662\n----------------------------------------\nINFO - Agent nfsp1, step 122368, sl-loss: 1.2066445350646973"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7b44d049cfca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Evaluate the performance. Play with random agents.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mevaluate_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtournament\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Close files in the logger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\somat\\Documents\\GitHub\\RL\\rlcard\\utils\\utils.py\u001b[0m in \u001b[0;36mtournament\u001b[1;34m(env, num)\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_payoffs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_payoffs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0m_p\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_payoffs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\somat\\Documents\\GitHub\\RL\\rlcard\\envs\\env.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, is_training)\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;31m# Agent plays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayer_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\somat\\Documents\\GitHub\\RL\\rlcard\\agents\\nfsp_agent_pytorch.py\u001b[0m in \u001b[0;36meval_step\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'obs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mlegal_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'legal_actions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_act\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_illegal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegal_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\somat\\Documents\\GitHub\\RL\\rlcard\\agents\\nfsp_agent_pytorch.py\u001b[0m in \u001b[0;36m_act\u001b[1;34m(self, info_state)\u001b[0m\n\u001b[0;32m    225\u001b[0m         '''\n\u001b[0;32m    226\u001b[0m         \u001b[0minfo_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[0minfo_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfo_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for episode in range(1):#episode_num):\n",
    "\n",
    "        # First sample a policy for the episode\n",
    "    for agent in agents:\n",
    "        agent.sample_episode_policy()\n",
    "\n",
    "        # Generate data from the environment\n",
    "    trajectories, _ = env.run(is_training=True)\n",
    "\n",
    "     # Feed transitions into agent memory, and train the agent\n",
    "    for i in range(env.player_num):\n",
    "        for ts in trajectories[i]:\n",
    "            agents[i].feed(ts)\n",
    "\n",
    "        # Evaluate the performance. Play with random agents.\n",
    "    if episode % evaluate_every == 0:\n",
    "        logger.log_performance(env.timestep, tournament(eval_env, evaluate_num)[0])\n",
    "\n",
    "    # Close files in the logger\n",
    "logger.close_files()\n",
    "\n",
    "    # Plot the learning curve\n",
    "logger.plot('NFSP')\n",
    "\n",
    "    # Save model\n",
    "save_dir = 'models/nolimit_holdem_nfsp_pytorch'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "state_dict = {}\n",
    "for agent in agents:\n",
    "    state_dict.update(agent.get_state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_dir = 'models/nolimit_holdem_nfsp_pytorch'\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "state_dict = {}\n",
    "for agent in agents:\n",
    "    state_dict.update(agent.get_state_dict())\n",
    "torch.save(state_dict, os.path.join(save_dir, 'model.pth'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bit8c6495d42d63455699a3c1e7b3c8e30a",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}